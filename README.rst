CorrectOCR
==========

.. image:: https://readthedocs.org/projects/correctocr/badge/?version=latest
   :target: https://correctocr.readthedocs.io/en/latest/?badge=latest
   :alt: Documentation Status

Introduction
============

CorrectOCR is a tool to post-process OCR text in order to improve its
quality, using a number of methods to minimize annotator work.

Usage
=====

Workflow
--------

Usage of CorrectOCR is divided into several successive tasks.

To train the software, one must first create or obtain set of matching
original uncorrected files with corresponding known-correct “gold”
files. Additionally, a dictionary of the target language is needed.

The pairs of (original, gold) files are then used to create *k*
replacement candidates for each token (word) in a new given file. A
number of heuristic decisions are configured based on whether a given
token is found in the dictionary, are the candidates preferable to the
original, etc. Finally, a CLI can be used by the annotator to select
among the candidates, and a new corrected file is generated.

When a corrected file is satisfactory, it can be moved or copied to the
gold directory and in turn be used to tune the HMM further, thus
improving the *k*-best candidates for subsequent files.

Configuration
-------------

When invoked, CorrectOCR looks for a file named ``CorrectOCR.ini`` in
the working directory. If found, it is loaded, and any entries will be
considered defaults to their corresponding option. For example:

.. code:: ini

   [configuration]
   characterSet = ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz

   [workspace]
   correctedPath = corrected/
   goldPath = gold/
   originalPath = original/
   trainingPath = training/
   nheaderlines = 0

   [resources]
   correctionTrackingFile = resources/correction_tracking.json
   dictionaryFile = resources/dictionary.txt
   hmmParamsFile = resources/hmm_parameters.json
   memoizedCorrectionsFile = resources/memoized_corrections.json
   multiCharacterErrorFile = resources/multicharacter_errors.json
   reportFile = resources/report.txt
   heuristicSettingsFile = resources/settings.json

By default, CorrectOCR requires 4 subdirectories in the working
directory, which will be used as the current ``Workspace``:

-  ``original/`` contains the original uncorrected files. If necessary,
   it can be configured with the ``--originalPath`` argument.
-  ``gold/`` contains the known correct “gold” files. If necessary, it
   can be configured with the ``--goldPath`` argument.
-  ``training/`` contains the various generated files used during
   training. If necessary, it can be configured with the
   ``--trainingPath`` argument.
-  ``corrected/`` contains the corrected files generated by running the
   ``correct`` command. If necessary, it can be configured with the
   ``--correctedPath`` argument.

Corresponding files in *original*, *gold*, and *corrected* are named
identically, and the filename without extension is considered the *file
ID*. The generated files in ``training/`` have suffixes according to
their kind.

If generated files exist, CorrectOCR will generally avoid doing
redundant calculations. The ``--force`` switch overrides this, forcing
CorrectOCR to create new files (after moving the existing ones out of
the way). Alternately, one may delete a subset of the generated files to
only recreate those.

The ``Workspace`` also has a ``ResourceManager`` (accessible in code via
``.resources``) that handles access to the dictionary, HMM parameter
files, etc.

Commands
--------

Commands and their arguments are called directly on the module, like so:

.. code:: console

   python -m CorrectOCR [command] [args...]

The following commands are available:

-  ``build_dictionary`` creates a dictionary. Input files can be either
   ``.pdf``, ``.txt``, or ``.xml`` (in `TEI
   format <https://en.wikipedia.org/wiki/Text_Encoding_Initiative>`__).
   They may be contained in ``.zip``-files.

   -  The ``--corpusPath`` option specifies a directory of files.
   -  The ``--corpusFile`` option specifies a file containing paths and
      URLs. One such file for a dictionary covering 1800–1948 Danish is
      provided under ``resources/``.

   It is strongly recommended to generate a large dictionary for best
   performance.

-  ``align`` aligns a pair of (original, gold) files in order to
   determine which characters and words were misread in the original and
   corrected in the gold.

   -  The ``--fileid`` option specifies a single pair of files to align.
   -  The ``--all`` option aligns all available pairs. Can be combined
      with ``--exclude`` to skip specific files.

-  ``build_model`` uses the alignments to create parameters for the HMM.

   -  The ``--smoothingParameter`` option can be adjusted as needed.

-  ``prepare`` prepares texts in preparation for corrections.

   -  The ``--fileid`` option specifies which file to tokenize.
   -  The ``--all`` option tokenizes all available texts. Can be
      combined with ``--exclude`` to skip specific files.
   -  The ``--step`` option specifies how many of the processing steps
      to take. The default is to take all steps.

      -  ``tokenize`` simply splits the text into tokens (words).
      -  ``align`` aligns tokens with gold versions, if these exist.
      -  ``kbest`` calculates *k*-best correction candidates for each
         token via the HMM.
      -  ``bin`` sorts the tokens into *bins* according to the
         `heuristics <#heuristics>`__ below.

      Each of the steps includes the previous step, and will save an
      intermediary CSV file containing information about each token.

-  ``stats`` is used to configure which decisions the program should
   make about each bin of tokens:

   -  ``--make_report`` generates a statistical report on whether
      originals/\ *k*-best equal are in the dictionary, etc. This report
      can then be inspected and annotated with the desired decision for
      each *bin*.
   -  ``--make_settings`` creates correction settings based on the
      annotated report.

-  ``correct`` uses the settings to sort the tokens into bins and makes
   automated decisions as configured.

   -  The ``--fileid`` option specifies which file to correct.

   There are three ways to run corrections:

   -  ``--interactive`` runs an interactive correction CLI for the
      remaining undecided tokens (see `Correction
      Interface <#correction-interace>`__ below).
   -  ``--apply`` takes a path argument to an edited token CSV file and
      applies the corrections therein.
   -  ``--autocorrect`` applies available corrections as configured in
      correction settings (ie. any heuristic bins not marked for human
      annotation).

-  ``index`` finds specified terms for use in index-generation.

   -  The ``--fileid`` option specifies a single file for which to
      generate an index.
   -  The ``--all`` option generates indices for all available files.
      Can be combined with ``--exclude`` to skip specific files.
   -  The ``--termFile`` option specifies a text file containing a word
      on each line, which will be matched against the tokens. The option
      may be repeated, and each filename (without extension) will be
      used as markers for the string.
   -  The ``--highlight`` option will create a copy of the input files
      with highlighted words (only available for PDFs).
   -  The ``--autocorrect`` option applies available corrections prior
      to search/highlighting, as above.

-  ``cleanup`` deletes the backup files in the training directory.

   -  The ``--dryrun`` option simply lists the files without actually
      deleting them.
   -  The ``--full`` option also deletes the current files (ie. those
      without .nnn. in their suffix).

Heuristics
----------

A given token and its *k*-best candidates are compared and checked with
the dictionary. Based on this, it is matched with a *bin*.

============================== = = = = = = = = =
bin                            1 2 3 4 5 6 7 8 9
============================== = = = = = = = = =
*k* = orig?                    T T T F F F F F F
orig in dict?                  T F F F F F T T T
top *k*-best in dict?          T F F T F F T F F
lower-ranked *k*-best in dict? – F T – F T – F T
============================== = = = = = = = = =

Each *bin* must be assigned a setting that determines what decision is
made:

-  ``o`` / *original*: select the original token as correct.
-  ``k`` / *kbest*: select the top *k*-best candidate as correct.
-  ``d`` / *kdict*: select the first lower-ranked candidate that is in
   the dictionary.
-  ``a`` / *annotator*: defer selection to annotator.

Once the report and settings are generated, it is not strictly necessary
to update them every single time the model is updated. It is however a
good idea to do it regularly as the corpus grows and more tokens become
available for the statistics.

Correction Interface
--------------------

The annotator will be presented with the tokens that match a heuristic
bin that was marked for annotation.

They may then enter a command. The commands reflect the above settings,
with an additional ``defer`` command to defer decision to a later time.

Prefixing the entered text with an exclamation point causes it to be
considered the corrected version of the token. For example, if the token
is “Wagor” and no suitable candidate is available, the annotator may
enter ``!Wagon`` to correct the word.

Corrections are memoized, so the file need not be corrected fully in one
session. To finish a session and save corrections, use the ``quit``
command.

A ``help`` command is available in the interface.

History
=======

CorrectOCR is based on code created by:

-  Caitlin Richter (ricca@seas.upenn.edu)
-  Matthew Wickes (wickesm@seas.upenn.edu)
-  Deniz Beser (dbeser@seas.upenn.edu)
-  Mitchell Marcus (mitch@cis.upenn.edu)

See their article *“Low-resource Post Processing of Noisy OCR Output for
Historical Corpus Digitisation”* (LREC-2018) for further details, it is
available online:
http://www.lrec-conf.org/proceedings/lrec2018/pdf/971.pdf

The original python 2.7 code (see ``original``-tag in the repository)
has been licensed under Creative Commons Attribution 4.0
(`CC-BY-4.0 <https://creativecommons.org/licenses/by/4.0/>`__, see also
``license.txt`` in the repository).

The code has subsequently been updated to Python 3 and further expanded
by Mikkel Eide Eriksen (mikkel.eriksen@gmail.com) for the `Copenhagen
City Archives <https://www.kbharkiv.dk/>`__ (mainly structural changes,
the algorithms are generally preserved as-is). Pull requests welcome!

Requirements
============

-  Python >= 3.6

For package dependencies see `requirements.txt <requirements.txt>`__.
They can be installed using ``pip install -r requirements.txt``
